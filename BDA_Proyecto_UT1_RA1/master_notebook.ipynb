{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9c185c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#001f3f; color:white; padding:15px; border-radius:8px;\">\n",
    "<h2 style=\"text-align:left;\">UT1 - BDA - Ejercicio Sensores de CO₂ — Borja Ramos Oliva</h2>\n",
    "<p style=\"font-size:16px;\">\n",
    "Este es el cuaderno en el que desarrollaremos el pipeline completo del proceso: ingesta micro-batch, limpieza, almacenamiento y reporte Markdown.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85278df2",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#001f3f; color:white; padding:15px; border-radius:8px;\">\n",
    "<h3 style=\"text-align:left;\">1. Introducción y Objetivos</h3>\n",
    "<p style=\"font-size:14px;\">\n",
    "En este cuaderno, realizaremos el ejercicio propuesto en unidad 1 de Big Data Aplicado (BDA). El objetivo principal es desarrollar un pipeline completo que abarque desde la ingesta de datos en micro-batch, pasando por la limpieza y almacenamiento de los mismos, hasta la generación de un reporte en formato Markdown. Utilizaremos datos de sensores de CO₂ para ilustrar cada etapa del proceso.\n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4327d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.1 Importamos las librerías necesarias\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import json\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.2 Definimos la configuración inicial de pandas\n",
    "pd.set_option(\"display.max_rows\", 10) # Mostrar todas las filas\n",
    "pd.set_option(\"display.max_columns\", None) # Mostrar todas las columnas\n",
    "pd.set_option(\"display.width\", 120) # Ajustar el ancho de la visualización\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88049cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estructura del proyecto creada o verificada ✅\n",
      "Raíz: /Users/b0rjen/Library/Mobile Documents/com~apple~CloudDocs/dev/BDA_Proyecto_UT1_RA1/project\n"
     ]
    }
   ],
   "source": [
    "### 1.3 Definimos constantes y parámetros del proyecto. Su uso se explica en el reporte final.\n",
    "### 1.4 Parámetros de calidad del aire según concentración de CO2 (ppm)\n",
    "BANDAS = {\n",
    "    \"verde\": (None, 700),     # buena calidad del aire\n",
    "    \"amarilla\": (700, 1000),  # media\n",
    "    \"roja\": (1000, None)      # mala (alerta)\n",
    "}\n",
    "\n",
    "HORARIO_LECTIVO = [(8, 14), (16, 21)] ### Definimos el horario lectivo, por ejemplo de 8 a 14h y de 16 a 21h\n",
    "\n",
    "### 1.5 Definimos las rutas de los directorios del proyecto, ingesta, almacenamiento y reporte\n",
    "\n",
    "# Generamos el directorio del proyecto si no existe con la siguiente línea (descomentarlo si es necesario)\n",
    "# !mkdir -p project\n",
    "\n",
    "ROOT = Path.cwd() / \"project\"\n",
    "DATA = ROOT / \"data\" / \"drops\"\n",
    "OUT  = ROOT / \"output\"\n",
    "PARQ = OUT / \"parquet\"\n",
    "QLTY = OUT / \"quality\"\n",
    "CKPT = OUT / \"checkpoints\"\n",
    "DB   = OUT / \"ut1.db\"\n",
    "REPORT = OUT / \"reporte.md\"\n",
    "\n",
    "for d in [DATA, OUT, PARQ, QLTY, CKPT]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Estructura del proyecto creada o verificada ✅\")\n",
    "print(f\"Raíz: {ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d0341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /Users/b0rjen/Library/Mobile Documents/com~apple~CloudDocs/dev/BDA_Proyecto_UT1_RA1\n",
      "Nota: notebook está en: /Users/b0rjen/Library/Mobile Documents/com~apple~CloudDocs/dev/BDA_Proyecto_UT1_RA1\n"
     ]
    }
   ],
   "source": [
    "### Podemos verificar el directorio (a mi me han fallado permisos en algunas ocasiones))\n",
    "# import os, pathlib\n",
    "# print(\"CWD:\", os.getcwd())\n",
    "# print(\"Nota: notebook está en:\", pathlib.Path().resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972fa12f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#001f3f; color:white; padding:15px; border-radius:8px;\">\n",
    "<h3 style=\"text-align:left;\">2. Generación / adquisición de datos</h3>\n",
    "<p style=\"font-size:14px;\">\n",
    "Vamos a generar un dataset simulado siguiendo la estructura que se da en el enunciado.\n",
    "\n",
    "En él, introduciremos una serie de inconsistencias que luego limpiaremos explicando el por qué. \n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6387d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1 Generaremos un NDJSON con los datos simulados en el directorio de ingesta: {DATA} (fuente bronce)\n",
    "### Se llamará \"lecturas.log\" en projecto/data/drops/lecturas.log\n",
    "from datetime import timedelta\n",
    "LOGFILE = DATA / \"lecturas.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5174351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Definimos la función para generar el event_id\n",
    "\n",
    "def generar_evento_id(ts_iso: str, aula: str) -> str:\n",
    "    \"\"\"\n",
    "    Genera un identificador único de evento (`event_id`) para cada lectura.\n",
    "\n",
    "    Args:\n",
    "        ts_iso (str): Marca temporal en formato ISO (por ejemplo '2025-01-03T09:00:00Z').\n",
    "        aula (str): Nombre o código del aula (por ejemplo 'A101').\n",
    "\n",
    "    Returns:\n",
    "        str: Identificador en formato 'aula-hhmmss', \n",
    "             por ejemplo 'a101-090000' para A101 a las 09:00:00.\n",
    "    \n",
    "    Ejemplo:\n",
    "        >>> gen_event_id(\"2025-01-03T09:00:00Z\", \"A101\")\n",
    "        'a101-090000'\n",
    "    \"\"\"\n",
    "    ts = pd.to_datetime(ts_iso, utc=True)\n",
    "    return f\"{str(aula).lower()}-{ts.strftime('%H%M%S')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generado NDJSON: /Users/b0rjen/Library/Mobile Documents/com~apple~CloudDocs/dev/BDA_Proyecto_UT1_RA1/project/data/drops/lecturas.log  (líneas ≈2160)\n"
     ]
    }
   ],
   "source": [
    "# 2.3 Definimos la función para simular lecturas de sensores de CO2 con anomalías\n",
    "\n",
    "def simular_lecturas(\n",
    "    fp: Path,\n",
    "    start_utc: datetime,\n",
    "    minutes: int = 180,\n",
    "    step_sec: int = 15,\n",
    "    aulas=(\"A101\", \"A102\", \"A103\"),\n",
    "    seed: int = 7,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Simula un conjunto de lecturas de sensores de CO₂ y las guarda en formato NDJSON.\n",
    "\n",
    "    Crea un archivo `.log` donde cada línea es un evento JSON con los campos:\n",
    "    - ts: marca temporal ISO (UTC)\n",
    "    - aula: identificador del aula\n",
    "    - co2_ppm: valor de concentración de CO₂ en ppm\n",
    "    - event_id: identificador único (aula-hhmmss)\n",
    "\n",
    "    Además, introduce intencionadamente anomalías para probar la limpieza posterior:\n",
    "    - Valores fuera de rango (co2_ppm < 0 o > 5000)\n",
    "    - Spikes (incrementos repentinos > 1000 ppm)\n",
    "    - Sensor atascado (valores constantes durante 5 min)\n",
    "    - Líneas malformadas o con valores nulos\n",
    "    - Registros fuera del horario lectivo (empieza 06:00Z)\n",
    "\n",
    "    Args:\n",
    "        fp (Path): Ruta de salida del archivo NDJSON.\n",
    "        start_utc (datetime): Fecha/hora inicial (en UTC).\n",
    "        minutes (int, optional): Duración total de la simulación en minutos. Por defecto 180 (3h).\n",
    "        step_sec (int, optional): Intervalo entre lecturas en segundos. Por defecto 15.\n",
    "        aulas (tuple[str], optional): Lista o tupla de aulas simuladas. Por defecto ('A101','A102','A103').\n",
    "        seed (int, optional): Semilla aleatoria para reproducibilidad. Por defecto 7.\n",
    "\n",
    "    Returns:\n",
    "        None: Escribe directamente en el archivo NDJSON especificado.\n",
    "\n",
    "    Ejemplo:\n",
    "        >>> simulate_lecturas(Path(\"lecturas.log\"), datetime(2025,1,3,6,0, tzinfo=timezone.utc))\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    ts = start_utc.replace(second=0, microsecond=0)\n",
    "\n",
    "    fp.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with fp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for i in range(int((minutes * 60) // step_sec)):\n",
    "            for aula in aulas:\n",
    "                # establecemos un patrón de clase (simula subida y bajada progresiva)\n",
    "                minute = (ts.minute + ts.hour * 60) % 60\n",
    "                if minute < 10:      base, noise = 500, 40\n",
    "                elif minute < 25:    base, noise = 900, 80\n",
    "                elif minute < 40:    base, noise = 1400, 120\n",
    "                elif minute < 50:    base, noise = 1800, 150\n",
    "                else:                base, noise = 900, 80\n",
    "\n",
    "                val = int(np.clip(rng.normal(base, noise), -2000, 7000))\n",
    "\n",
    "                # aquí creamos anomalías intencionadas\n",
    "                r = rng.random()\n",
    "                if r < 0.01:       # rango bajo\n",
    "                    val = -50\n",
    "                elif r < 0.02:     # rango alto\n",
    "                    val = 7000\n",
    "                if rng.random() < 0.01:  # aquí un \"spike\" positivo\n",
    "                    val = int(np.clip(val + rng.choice([1500, 2000]), -2000, 7000))\n",
    "\n",
    "                # sensor atascao , no varía en 5min. (20 lecturas de 15s)\n",
    "                if aula == \"A102\" and 60*60 <= i*step_sec < 60*60 + 5*60:\n",
    "                    val = 800\n",
    "\n",
    "                rec = {\n",
    "                    \"ts\": ts.isoformat().replace(\"+00:00\", \"Z\"),\n",
    "                    \"aula\": aula,\n",
    "                    \"co2_ppm\": val,\n",
    "                    \"event_id\": generar_evento_id(ts.isoformat(), aula),\n",
    "                }\n",
    "                f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "            # líneas malformadas y nulos\n",
    "            if i == 45:\n",
    "                f.write('{\"ts\": \"2025-01-03T08:45:00Z\", \"aula\": , \"co2_ppm\": 800}\\n') # falta aula\n",
    "            if i == 46:\n",
    "                f.write('{\"ts\":\"2025-01-03T08:46:00Z\",\"aula\":\"\",\"co2_ppm\":null,\"event_id\":\"mal-084600\"}\\n') # aula vacía, co2_ppm nulo\n",
    "\n",
    "            ts += timedelta(seconds=step_sec) # avanzamos el tiempo \n",
    "\n",
    "    print(f\"Generado NDJSON: {fp}  (líneas ≈{(minutes*60)//step_sec * len(aulas)})\") # número aproximado de líneas \n",
    "\n",
    "\n",
    "# Ejecutar solo si no existe , porque si no, se sobreescribe\n",
    "if not LOGFILE.exists():\n",
    "    simular_lecturas(\n",
    "        LOGFILE,\n",
    "        start_utc=datetime(2025, 1, 3, 6, 0, tzinfo=timezone.utc),  # incluye tramo fuera de horario\n",
    "        minutes=180,\n",
    "        step_sec=15,\n",
    "    )\n",
    "else:\n",
    "    print(f\"Ya hay un archivo: {LOGFILE} en el directorio y no se regenerará otro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e50195",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#001f3f; color:white; padding:15px; border-radius:8px;\">\n",
    "<h3 style=\"text-align:left;\">3. Ingesta de datos</h3>\n",
    "<p style=\"font-size:14px;\">\n",
    "Preparamos las tablas de SQLite y preparamos la ingesta para que el programa lea solo las líneas nuevas del .log\n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1 Creación de la base de datos SQLite y las tablas necesarias para almacenar los datos procesados y helpers\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def ensure_db(db_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Crea (si no existen) las tablas de trabajo:\n",
    "      - raw_events: eventos tal cual llegan (idempotencia por event_id)\n",
    "      - clean_events: se poblará en la fase de limpieza\n",
    "      - quarantine: filas apartadas con causa de calidad\n",
    "    Política de idempotencia: 'último gana' por _ingest_ts (UPSERT).\n",
    "    \"\"\"\n",
    "    with sqlite3.connect(db_path) as con:\n",
    "        cur = con.cursor()\n",
    "        # raw_events: trazabilidad mínima + event_id único\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS raw_events (\n",
    "            ts TEXT,\n",
    "            aula TEXT,\n",
    "            co2_ppm REAL,\n",
    "            event_id TEXT PRIMARY KEY,\n",
    "            _ingest_ts TEXT NOT NULL,\n",
    "            _source_file TEXT,\n",
    "            _pos INTEGER\n",
    "        );\n",
    "        \"\"\")\n",
    "        # clean_events: mismo esquema que raw + campos derivados (se llenará después)\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS clean_events (\n",
    "            ts TEXT,\n",
    "            aula TEXT,\n",
    "            co2_ppm REAL,\n",
    "            event_id TEXT PRIMARY KEY,\n",
    "            _ingest_ts TEXT NOT NULL,\n",
    "            _source_file TEXT,\n",
    "            _pos INTEGER\n",
    "        );\n",
    "        \"\"\")\n",
    "        # quarantine: filas con problemas de calidad + causa\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS quarantine (\n",
    "            ts TEXT,\n",
    "            aula TEXT,\n",
    "            co2_ppm REAL,\n",
    "            event_id TEXT,\n",
    "            cause TEXT NOT NULL,\n",
    "            _ingest_ts TEXT NOT NULL,\n",
    "            _source_file TEXT,\n",
    "            _pos INTEGER\n",
    "        );\n",
    "        \"\"\")\n",
    "        con.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ad70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLite listo ✅ → /Users/b0rjen/Library/Mobile Documents/com~apple~CloudDocs/dev/BDA_Proyecto_UT1_RA1/project/output/ut1.db\n"
     ]
    }
   ],
   "source": [
    "# 3.2 Definimos la función para insertar datos en raw_events con idempotencia\n",
    "\n",
    "def upsert_raw(df: pd.DataFrame, db_path: Path) -> Tuple[int,int]:\n",
    "    \"\"\"\n",
    "    Inserta df en raw_events con idempotencia por event_id.\n",
    "    UPSERT: si event_id existe y el nuevo _ingest_ts es mayor, actualiza.\n",
    "    Devuelve (insertados_nuevos, actualizados).\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return (0, 0)\n",
    "    with sqlite3.connect(db_path) as con:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "        sql = \"\"\"\n",
    "        INSERT INTO raw_events (ts, aula, co2_ppm, event_id, _ingest_ts, _source_file, _pos)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        ON CONFLICT(event_id) DO UPDATE SET\n",
    "            ts=excluded.ts,\n",
    "            aula=excluded.aula,\n",
    "            co2_ppm=excluded.co2_ppm,\n",
    "            _ingest_ts=excluded._ingest_ts,\n",
    "            _source_file=excluded._source_file,\n",
    "            _pos=excluded._pos\n",
    "        WHERE excluded._ingest_ts > raw_events._ingest_ts;\n",
    "        \"\"\"\n",
    "        before = cur.execute(\"SELECT COUNT(*) FROM raw_events;\").fetchone()[0]\n",
    "        updated = 0\n",
    "        for row in df[[\"ts\",\"aula\",\"co2_ppm\",\"event_id\",\"_ingest_ts\",\"_source_file\",\"_pos\"]].itertuples(index=False):\n",
    "            cur.execute(sql, row)\n",
    "            # sqlite no da updated_count por fila fácilmente; estimamos al final\n",
    "        con.commit()\n",
    "        after = cur.execute(\"SELECT COUNT(*) FROM raw_events;\").fetchone()[0]\n",
    "        inserted = max(after - before, 0)\n",
    "        # Aproximación de actualizados: total operaciones - insertados (cuando hay colisiones)\n",
    "        # (si quieres precisión fina, puedes comparar _ingest_ts previos vs nuevos)\n",
    "        updated = len(df) - inserted\n",
    "        return (inserted, updated)\n",
    "\n",
    "ensure_db(DB)\n",
    "print(\"SQLite listo ✅ →\", DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aae423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Definimos la función para leer solo las nuevas líneas del NDJSON con checkpointing\n",
    "\n",
    "def read_new_lines(fp: Path, checkpoint_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lee SOLO las nuevas líneas de un NDJSON (apoyándose en un checkpoint .offset),\n",
    "    parsea cada línea a dict y añade trazabilidad:\n",
    "      - _ingest_ts: timestamp UTC de la ingesta\n",
    "      - _source_file: nombre del archivo fuente\n",
    "      - _pos: offset (byte) de inicio de línea\n",
    "    Las líneas malformadas se devuelven como registros con ts/aula/co2_ppm/event_id = None\n",
    "    y cause='malformed' (esto permite mandarlas luego a quarantine en la fase de limpieza).\n",
    "\n",
    "    Args:\n",
    "        fp (Path): ruta del archivo NDJSON (ej. project/data/drops/lecturas.log)\n",
    "        checkpoint_dir (Path): carpeta donde guardar/leer el .offset\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: filas nuevas con trazabilidad (puede contener registros 'malformed')\n",
    "    \"\"\"\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ck = checkpoint_dir / (fp.name + \".offset\")\n",
    "\n",
    "    start = int(ck.read_text()) if ck.exists() else 0\n",
    "    rows = []\n",
    "\n",
    "    with fp.open(\"rb\") as f:\n",
    "        f.seek(start)\n",
    "        while True:\n",
    "            pos = f.tell()\n",
    "            b = f.readline()\n",
    "            if not b:\n",
    "                break\n",
    "            raw = b.decode(\"utf-8\", errors=\"replace\").strip()\n",
    "            try:\n",
    "                rec = json.loads(raw)\n",
    "            except Exception:\n",
    "                rec = {\"ts\": None, \"aula\": None, \"co2_ppm\": None, \"event_id\": None, \"cause\": \"malformed\"}\n",
    "            rec[\"_ingest_ts\"] = pd.Timestamp.utcnow().isoformat()\n",
    "            rec[\"_source_file\"] = fp.name\n",
    "            rec[\"_pos\"] = pos\n",
    "            rows.append(rec)\n",
    "\n",
    "        # guarda nuevo offset\n",
    "        ck.write_text(str(f.tell()))\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b135cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevas filas leídas: 2162 (incluyendo posibles 'malformed')\n",
      "UPSERT raw_events → insertadas: 2161, actualizadas: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>aula</th>\n",
       "      <th>co2_ppm</th>\n",
       "      <th>event_id</th>\n",
       "      <th>_ingest_ts</th>\n",
       "      <th>_source_file</th>\n",
       "      <th>_pos</th>\n",
       "      <th>cause</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-03T06:00:00Z</td>\n",
       "      <td>A101</td>\n",
       "      <td>500.0</td>\n",
       "      <td>a101-060000</td>\n",
       "      <td>2025-11-10T10:58:54.502987+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-03T06:00:00Z</td>\n",
       "      <td>A102</td>\n",
       "      <td>464.0</td>\n",
       "      <td>a102-060000</td>\n",
       "      <td>2025-11-10T10:58:54.504524+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-03T06:00:00Z</td>\n",
       "      <td>A103</td>\n",
       "      <td>502.0</td>\n",
       "      <td>a103-060000</td>\n",
       "      <td>2025-11-10T10:58:54.504558+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>180</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-03T06:00:15Z</td>\n",
       "      <td>A101</td>\n",
       "      <td>475.0</td>\n",
       "      <td>a101-060015</td>\n",
       "      <td>2025-11-10T10:58:54.504574+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>270</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-03T06:00:15Z</td>\n",
       "      <td>A102</td>\n",
       "      <td>504.0</td>\n",
       "      <td>a102-060015</td>\n",
       "      <td>2025-11-10T10:58:54.504586+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>360</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-01-03T06:00:15Z</td>\n",
       "      <td>A103</td>\n",
       "      <td>527.0</td>\n",
       "      <td>a103-060015</td>\n",
       "      <td>2025-11-10T10:58:54.504602+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>450</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-01-03T06:00:30Z</td>\n",
       "      <td>A101</td>\n",
       "      <td>423.0</td>\n",
       "      <td>a101-060030</td>\n",
       "      <td>2025-11-10T10:58:54.504627+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>540</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-01-03T06:00:30Z</td>\n",
       "      <td>A102</td>\n",
       "      <td>490.0</td>\n",
       "      <td>a102-060030</td>\n",
       "      <td>2025-11-10T10:58:54.504643+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>630</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ts  aula  co2_ppm     event_id                        _ingest_ts  _source_file  _pos cause\n",
       "0  2025-01-03T06:00:00Z  A101    500.0  a101-060000  2025-11-10T10:58:54.502987+00:00  lecturas.log     0   NaN\n",
       "1  2025-01-03T06:00:00Z  A102    464.0  a102-060000  2025-11-10T10:58:54.504524+00:00  lecturas.log    90   NaN\n",
       "2  2025-01-03T06:00:00Z  A103    502.0  a103-060000  2025-11-10T10:58:54.504558+00:00  lecturas.log   180   NaN\n",
       "3  2025-01-03T06:00:15Z  A101    475.0  a101-060015  2025-11-10T10:58:54.504574+00:00  lecturas.log   270   NaN\n",
       "4  2025-01-03T06:00:15Z  A102    504.0  a102-060015  2025-11-10T10:58:54.504586+00:00  lecturas.log   360   NaN\n",
       "5  2025-01-03T06:00:15Z  A103    527.0  a103-060015  2025-11-10T10:58:54.504602+00:00  lecturas.log   450   NaN\n",
       "6  2025-01-03T06:00:30Z  A101    423.0  a101-060030  2025-11-10T10:58:54.504627+00:00  lecturas.log   540   NaN\n",
       "7  2025-01-03T06:00:30Z  A102    490.0  a102-060030  2025-11-10T10:58:54.504643+00:00  lecturas.log   630   NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.4 Definimos la función principal de ingesta de un micro-lote\n",
    "\n",
    "def ingest_microbatch(logfile: Path, checkpoint_dir: Path, db_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Orquesta la ingesta de un micro-lote:\n",
    "      1) Lee nuevas líneas con checkpoint (read_new_lines)\n",
    "      2) Separa las 'malformed' (para quarantine en la siguiente fase)\n",
    "      3) UPSERT en raw_events (idempotencia por event_id, 'último gana')\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las filas parseadas (incluye posibles 'malformed')\n",
    "    \"\"\"\n",
    "    df_raw = read_new_lines(logfile, checkpoint_dir)\n",
    "    if df_raw.empty:\n",
    "        print(\"No hay líneas nuevas en el log. ✅\")\n",
    "        return df_raw\n",
    "\n",
    "    # Vista rápida\n",
    "    print(f\"Nuevas filas leídas: {len(df_raw)} (incluyendo posibles 'malformed')\")\n",
    "\n",
    "    # Inserta SOLO las que tienen event_id (las malformed irán a quarantine en limpieza)\n",
    "    df_valid = df_raw[df_raw[\"event_id\"].notna()].copy()\n",
    "    ins, upd = upsert_raw(df_valid, db_path)\n",
    "    print(f\"UPSERT raw_events → insertadas: {ins}, actualizadas: {upd}\")\n",
    "\n",
    "    # Nota: las 'malformed' las apartaremos en la fase de limpieza (quarantine)\n",
    "    return df_raw\n",
    "\n",
    "# Ejecuta la ingesta del micro-lote actual\n",
    "df_raw_batch = ingest_microbatch(LOGFILE, CKPT, DB)\n",
    "df_raw_batch.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PONER EN MODO TABLA para el reporte final\n",
    "\n",
    "# | Campo       | Descripción                                                                 |\n",
    "# |-------------|-----------------------------------------------------------------------------|\n",
    "# | ts          | marca temporal original del evento (UTC, formato ISO)                      |\n",
    "# | aula       | aula simulada (A101, A102, A103)                                           |\n",
    "# | co2_ppm    | valor de concentración en ppm (float)                                      |\n",
    "# | event_id   | identificador único del evento (clave natural, usada para idempotencia)    |\n",
    "# | _ingest_ts | momento en que el lote fue procesado (ahora real, 2025-11-10…)            |\n",
    "# | _source_file| archivo fuente (lecturas.log)                                             |\n",
    "# | _pos       | posición de lectura en bytes dentro del archivo (checkpoint)               |\n",
    "# | cause      | nula por ahora → se rellenará en limpieza cuando detectemos anomalías      |\n",
    "\n",
    "# aula simulada (A101, A102, A103)\n",
    "# co2_ppm\n",
    "# valor de concentración en ppm (float)\n",
    "# event_id\n",
    "# identificador único del evento (clave natural, usada para idempotencia)\n",
    "# _ingest_ts\n",
    "# momento en que el lote fue procesado (ahora real, 2025-11-10…)\n",
    "# _source_file\n",
    "# archivo fuente (lecturas.log)\n",
    "# _pos\n",
    "# posición de lectura en bytes dentro del archivo (checkpoint)\n",
    "# cause\n",
    "# nula por ahora → se rellenará en limpieza cuando detectemos anomalías\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da3ef9c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#001f3f; color:white; padding:15px; border-radius:8px;\">\n",
    "<h3 style=\"text-align:left;\">4. Limpieza de datos y valoración de calidad. Exportación a Parquet</h3>\n",
    "<p style=\"font-size:14px;\">\n",
    "A partir de la ingesta anteriormente explicada, exportamos tanto datos crudos como limpios\n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630b151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/_bgf9xt50ll121jxkyy_ckdm0000gn/T/ipykernel_40222/2509549627.py:40: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'null_value' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df[\"co2_ppm\"].isna(), \"cause\"] = \"null_value\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado en SQLite → clean: 702, quarantine: 1459\n",
      "Exportado a Parquet (raw) → /Users/b0rjen/Library/Mobile Documents/com~apple~CloudDocs/dev/BDA_Proyecto_UT1_RA1/project/output/parquet/raw\n",
      "Exportado a Parquet (clean) → /Users/b0rjen/Library/Mobile Documents/com~apple~CloudDocs/dev/BDA_Proyecto_UT1_RA1/project/output/parquet/clean\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Definimos la función para limpiar y exportar datos a Parquet\n",
    "\n",
    "def clean_and_export(db_path: Path, parquet_dir: Path) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Lee los datos crudos (raw_events) desde SQLite, detecta anomalías y genera:\n",
    "      - clean_events  → válidos, listos para análisis\n",
    "      - quarantine    → inválidos con su 'cause' documentada\n",
    "    Exporta ambos a Parquet particionado por fecha/hora (bronze y silver).\n",
    "\n",
    "    Reglas aplicadas:\n",
    "      • co2_ppm < 300 o > 5000 → out_of_range\n",
    "      • aula vacío → missing_aula\n",
    "      • ts fuera de horario lectivo → out_of_hours (08–14 y 16–21)\n",
    "      • registros con 'malformed' → malformed\n",
    "\n",
    "    Args:\n",
    "        db_path (Path): Ruta a la base de datos SQLite.\n",
    "        parquet_dir (Path): Carpeta base para Parquet (output/parquet/)\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: (df_clean, df_quarantine)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    with sqlite3.connect(db_path) as con:\n",
    "        df = pd.read_sql_query(\"SELECT * FROM raw_events;\", con)\n",
    "\n",
    "    # Asegurar tipado\n",
    "    df[\"ts\"] = pd.to_datetime(df[\"ts\"], errors=\"coerce\", utc=True)\n",
    "    df[\"co2_ppm\"] = pd.to_numeric(df[\"co2_ppm\"], errors=\"coerce\")\n",
    "    df[\"aula\"] = df[\"aula\"].astype(\"string\")\n",
    "\n",
    "    # Definimos el horario válido (08–14 y 16–21)\n",
    "    def in_valid_hours(ts):\n",
    "        if pd.isna(ts):\n",
    "            return False\n",
    "        h = ts.hour\n",
    "        return (8 <= h < 14) or (16 <= h < 21)\n",
    "\n",
    "    # Detectar causas\n",
    "    df[\"cause\"] = np.nan\n",
    "    df.loc[df[\"co2_ppm\"].isna(), \"cause\"] = \"null_value\"\n",
    "    df.loc[df[\"aula\"].isna() | (df[\"aula\"].str.strip() == \"\"), \"cause\"] = \"missing_aula\"\n",
    "    df.loc[(df[\"co2_ppm\"] < 300) | (df[\"co2_ppm\"] > 5000), \"cause\"] = \"out_of_range\"\n",
    "    df.loc[~df[\"ts\"].apply(in_valid_hours), \"cause\"] = df[\"cause\"].fillna(\"out_of_hours\")\n",
    "    df.loc[df[\"event_id\"].isna(), \"cause\"] = \"malformed\"\n",
    "\n",
    "    # Separamos limpios / cuarentena\n",
    "    df_clean = df[df[\"cause\"].isna()].copy()\n",
    "    df_quar = df[~df[\"cause\"].isna()].copy()\n",
    "\n",
    "    # Guardamos en SQLite\n",
    "    with sqlite3.connect(db_path) as con:\n",
    "        df_clean.to_sql(\"clean_events\", con, if_exists=\"replace\", index=False)\n",
    "        df_quar.to_sql(\"quarantine\", con, if_exists=\"replace\", index=False)\n",
    "        con.commit()\n",
    "\n",
    "    print(f\"Guardado en SQLite → clean: {len(df_clean)}, quarantine: {len(df_quar)}\")\n",
    "\n",
    "    ### EXPORTAMOS A PARQUET ###\n",
    "    for name, data in {\"raw\": df, \"clean\": df_clean}.items():\n",
    "        if data.empty:\n",
    "            continue\n",
    "        data = data.copy()\n",
    "        data[\"year\"] = data[\"ts\"].dt.year\n",
    "        data[\"month\"] = data[\"ts\"].dt.month\n",
    "        data[\"day\"] = data[\"ts\"].dt.day\n",
    "        data[\"hour\"] = data[\"ts\"].dt.hour\n",
    "\n",
    "        outdir = parquet_dir / name\n",
    "        data.to_parquet(\n",
    "            outdir,\n",
    "            partition_cols=[\"year\", \"month\", \"day\", \"hour\"],\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"Exportado a Parquet ({name}) → {outdir}\")\n",
    "\n",
    "    return df_clean, df_quar\n",
    "\n",
    "\n",
    "### Ejecutamos la limpieza completa\n",
    "df_clean, df_quar = clean_and_export(DB, PARQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b44f83f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo datos limpios:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>aula</th>\n",
       "      <th>co2_ppm</th>\n",
       "      <th>event_id</th>\n",
       "      <th>_ingest_ts</th>\n",
       "      <th>_source_file</th>\n",
       "      <th>_pos</th>\n",
       "      <th>cause</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>2025-01-03 08:00:00+00:00</td>\n",
       "      <td>A102</td>\n",
       "      <td>510.0</td>\n",
       "      <td>a102-080000</td>\n",
       "      <td>2025-11-10T10:58:54.515738+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>130495</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>2025-01-03 08:00:00+00:00</td>\n",
       "      <td>A103</td>\n",
       "      <td>501.0</td>\n",
       "      <td>a103-080000</td>\n",
       "      <td>2025-11-10T10:58:54.515743+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>130585</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>2025-01-03 08:00:15+00:00</td>\n",
       "      <td>A101</td>\n",
       "      <td>465.0</td>\n",
       "      <td>a101-080015</td>\n",
       "      <td>2025-11-10T10:58:54.515747+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>130675</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>2025-01-03 08:00:15+00:00</td>\n",
       "      <td>A102</td>\n",
       "      <td>520.0</td>\n",
       "      <td>a102-080015</td>\n",
       "      <td>2025-11-10T10:58:54.515752+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>130765</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>2025-01-03 08:00:15+00:00</td>\n",
       "      <td>A103</td>\n",
       "      <td>550.0</td>\n",
       "      <td>a103-080015</td>\n",
       "      <td>2025-11-10T10:58:54.515756+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>130855</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ts  aula  co2_ppm     event_id                        _ingest_ts  _source_file    _pos  \\\n",
       "1442 2025-01-03 08:00:00+00:00  A102    510.0  a102-080000  2025-11-10T10:58:54.515738+00:00  lecturas.log  130495   \n",
       "1443 2025-01-03 08:00:00+00:00  A103    501.0  a103-080000  2025-11-10T10:58:54.515743+00:00  lecturas.log  130585   \n",
       "1444 2025-01-03 08:00:15+00:00  A101    465.0  a101-080015  2025-11-10T10:58:54.515747+00:00  lecturas.log  130675   \n",
       "1445 2025-01-03 08:00:15+00:00  A102    520.0  a102-080015  2025-11-10T10:58:54.515752+00:00  lecturas.log  130765   \n",
       "1446 2025-01-03 08:00:15+00:00  A103    550.0  a103-080015  2025-11-10T10:58:54.515756+00:00  lecturas.log  130855   \n",
       "\n",
       "     cause  \n",
       "1442   NaN  \n",
       "1443   NaN  \n",
       "1444   NaN  \n",
       "1445   NaN  \n",
       "1446   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo quarantine:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>aula</th>\n",
       "      <th>co2_ppm</th>\n",
       "      <th>event_id</th>\n",
       "      <th>_ingest_ts</th>\n",
       "      <th>_source_file</th>\n",
       "      <th>_pos</th>\n",
       "      <th>cause</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-03 06:00:00+00:00</td>\n",
       "      <td>A101</td>\n",
       "      <td>500.0</td>\n",
       "      <td>a101-060000</td>\n",
       "      <td>2025-11-10T10:58:54.502987+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>0</td>\n",
       "      <td>out_of_hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-03 06:00:00+00:00</td>\n",
       "      <td>A102</td>\n",
       "      <td>464.0</td>\n",
       "      <td>a102-060000</td>\n",
       "      <td>2025-11-10T10:58:54.504524+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>90</td>\n",
       "      <td>out_of_hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-03 06:00:00+00:00</td>\n",
       "      <td>A103</td>\n",
       "      <td>502.0</td>\n",
       "      <td>a103-060000</td>\n",
       "      <td>2025-11-10T10:58:54.504558+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>180</td>\n",
       "      <td>out_of_hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-03 06:00:15+00:00</td>\n",
       "      <td>A101</td>\n",
       "      <td>475.0</td>\n",
       "      <td>a101-060015</td>\n",
       "      <td>2025-11-10T10:58:54.504574+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>270</td>\n",
       "      <td>out_of_hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-03 06:00:15+00:00</td>\n",
       "      <td>A102</td>\n",
       "      <td>504.0</td>\n",
       "      <td>a102-060015</td>\n",
       "      <td>2025-11-10T10:58:54.504586+00:00</td>\n",
       "      <td>lecturas.log</td>\n",
       "      <td>360</td>\n",
       "      <td>out_of_hours</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ts  aula  co2_ppm     event_id                        _ingest_ts  _source_file  _pos  \\\n",
       "0 2025-01-03 06:00:00+00:00  A101    500.0  a101-060000  2025-11-10T10:58:54.502987+00:00  lecturas.log     0   \n",
       "1 2025-01-03 06:00:00+00:00  A102    464.0  a102-060000  2025-11-10T10:58:54.504524+00:00  lecturas.log    90   \n",
       "2 2025-01-03 06:00:00+00:00  A103    502.0  a103-060000  2025-11-10T10:58:54.504558+00:00  lecturas.log   180   \n",
       "3 2025-01-03 06:00:15+00:00  A101    475.0  a101-060015  2025-11-10T10:58:54.504574+00:00  lecturas.log   270   \n",
       "4 2025-01-03 06:00:15+00:00  A102    504.0  a102-060015  2025-11-10T10:58:54.504586+00:00  lecturas.log   360   \n",
       "\n",
       "          cause  \n",
       "0  out_of_hours  \n",
       "1  out_of_hours  \n",
       "2  out_of_hours  \n",
       "3  out_of_hours  \n",
       "4  out_of_hours  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Vamos a imprimir los resultados\n",
    "print(\"\\nEjemplo datos limpios:\")\n",
    "display(df_clean.head(5))\n",
    "\n",
    "print(\"\\nEjemplo quarantine:\")\n",
    "display(df_quar.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "750a94c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cause\n",
       "out_of_hours    1408\n",
       "out_of_range      50\n",
       "missing_aula       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Vamos a ver qué errores hay en quarantine, registros en aula y causa.\n",
    "df_quar[\"cause\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2e3e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aula\n",
       "A101    235\n",
       "A103    234\n",
       "A102    233\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Veamos también el recuento de lecturas limpias por aula\n",
    "df_clean[\"aula\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fda413c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#001f3f; color:white; padding:15px; border-radius:8px;\">\n",
    "<h3 style=\"text-align:left;\">5. Otras opciones de control y mantenimiento</h3>\n",
    "<p style=\"font-size:14px;\">\n",
    "Con estas funciones podemos limpiar para agilizar consultas en la BBDD y resetear el pipeline\n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b0418",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.1 Optimización de consultas en SQLite (OPCIONAL)\n",
    "\n",
    "with sqlite3.connect(DB) as con:\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"CREATE INDEX IF NOT EXISTS ix_raw_ts ON raw_events(ts);\")\n",
    "    cur.execute(\"CREATE INDEX IF NOT EXISTS ix_clean_ts_aula ON clean_events(ts, aula);\")\n",
    "    con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc816bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sqlite_autoindex_raw_events_1', 'raw_events'), ('ix_raw_ts', 'raw_events'), ('ix_clean_ts_aula', 'clean_events')]\n"
     ]
    }
   ],
   "source": [
    "# 5.1.1: Listar índices en la base de datos\n",
    "with sqlite3.connect(DB) as con:\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"SELECT name, tbl_name FROM sqlite_master WHERE type='index';\")\n",
    "    print(cur.fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ba954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0, 164, 'SEARCH clean_events USING INDEX ix_clean_ts_aula (ts>? AND ts<?)')]\n"
     ]
    }
   ],
   "source": [
    "# 5.1.2: plan de consulta para una búsqueda común\n",
    "with sqlite3.connect(DB) as con:\n",
    "    cur = con.cursor()\n",
    "    query = \"SELECT * FROM clean_events WHERE aula='A101' AND ts BETWEEN '2025-01-03T08:00' AND '2025-01-03T09:00';\"\n",
    "    plan = cur.execute(\"EXPLAIN QUERY PLAN \" + query).fetchall()\n",
    "print(plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56980470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Helper para limpiar la base de datos.\n",
    "def clear_database(db_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Elimina todas las tablas de la base de datos SQLite.\n",
    "    Útil para resetear el pipeline y empezar de nuevo.\n",
    "\n",
    "    Args:\n",
    "        db_path (Path): Ruta a la base de datos SQLite.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with sqlite3.connect(db_path) as con:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"DROP TABLE IF EXISTS raw_events;\")\n",
    "        cur.execute(\"DROP TABLE IF EXISTS clean_events;\")\n",
    "        cur.execute(\"DROP TABLE IF EXISTS quarantine;\")\n",
    "        con.commit()\n",
    "    print(f\"Base de datos limpiada: {db_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Reseteo de pipeline\n",
    "def reset_pipeline():\n",
    "    for p in [PARQ, QLTY, CKPT]:\n",
    "        if p.exists():\n",
    "            for x in p.rglob(\"*\"):\n",
    "                if x.is_file(): x.unlink()\n",
    "    if DB.exists(): DB.unlink()\n",
    "    print(\"Reseteado output/, checkpoints/ y ut1.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94462012",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reset_pipeline() --> Descomentar para resetear todo el pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ut1_co2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
